{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bab868e",
   "metadata": {},
   "source": [
    "# Training Baseline Classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "576f3073",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install setfit==0.6.0\n",
    "!pip install openpyxl\n",
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d524c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from sentence_transformers.losses import CosineSimilarityLoss\n",
    "from setfit import SetFitTrainer\n",
    "from datasets import Dataset\n",
    "from setfit import sample_dataset\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import joblib\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "## Workaround for dashes in name\n",
    "from importlib import import_module\n",
    "nlbse_statistics = import_module('nlbse_statistics') \n",
    "\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5082794a",
   "metadata": {},
   "source": [
    "<a id='data_collection'></a>\n",
    "## Data collection\n",
    "We first load the data. \n",
    "For each language we create a dataset for each of the seperate category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cca0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = ['java', 'python', 'pharo']\n",
    "lan_cats = []\n",
    "datasets = {}\n",
    "for lan in langs: # for each language\n",
    "    df = pd.read_csv(f'./{lan}/input/{lan}.csv')\n",
    "    df['combo'] = df[['comment_sentence', 'class']].agg(' | '.join, axis=1)\n",
    "    df['label'] = df.instance_type\n",
    "    cats = list(map(lambda x: lan + '_' + x, list(set(df.category))))\n",
    "    lan_cats = lan_cats + cats\n",
    "    for cat in list(set(df.category)): # for each category\n",
    "        filtered =  df[df.category == cat]\n",
    "        train_data = Dataset.from_pandas(filtered[filtered.partition == 0])\n",
    "        test_data = Dataset.from_pandas(filtered[filtered.partition == 1])\n",
    "        datasets[f'{lan}_{cat}'] = {'train_data': train_data, 'test_data' : test_data}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4c29938",
   "metadata": {},
   "source": [
    "<a id='load_model'></a>\n",
    "\n",
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1245d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: We tweaked the hyperparams because of the dataset size of Java\n",
    "# num_itertations is very sensitive, be careful when tweaking\n",
    "hyperparameters = hyperparameters={'learning_rate': 1.7094555110821448e-05, 'num_epochs': 3, \n",
    "                                   'batch_size': 8, 'seed': 11, 'num_iterations': 10, \n",
    "                                   'max_iter': 241, 'solver': 'lbfgs'}\n",
    "    \n",
    "def model_init(params):\n",
    "    params = params or {}\n",
    "    max_iter = params.get(\"max_iter\", 100)\n",
    "    solver = params.get(\"solver\", \"liblinear\")\n",
    "    params = {\n",
    "        \"head_params\": {\n",
    "            \"max_iter\": max_iter,\n",
    "            \"solver\": solver,\n",
    "        }\n",
    "    }\n",
    "    return SetFitModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\", **params)\n",
    "\n",
    "# Create a fresh trainer with hyperparams\n",
    "def load_trainer(train_data, test_data):\n",
    "    trainer = SetFitTrainer(\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=test_data,\n",
    "        loss_class=CosineSimilarityLoss,\n",
    "        model_init=model_init,\n",
    "        column_mapping={\"combo\": \"text\", \"label\": \"label\"},\n",
    "    )\n",
    "\n",
    "    trainer.apply_hyperparameters(hyperparameters, final_model=True)\n",
    "    \n",
    "    return trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "249b020d",
   "metadata": {},
   "source": [
    "<a id='train_model'></a>\n",
    "\n",
    "\n",
    "## Train Models\n",
    "Train and save a model for each of the categories\n",
    "\n",
    "This will take around 3h per category, so around 2 days in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a865dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for each cat\n",
    "for lan_cat in lan_cats:\n",
    "    print(f'training {lan_cat}')\n",
    "    train_data = datasets[lan_cat]['train_data']\n",
    "    test_data = datasets[lan_cat]['test_data']\n",
    "    trainer = load_trainer(train_data, test_data)\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    joblib.dump(trainer, f'./models/{lan_cat}_all-mpnet-base-v2.joblib')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab3ba49a",
   "metadata": {},
   "source": [
    "<a id='eval'></a>\n",
    "\n",
    "## Evaluation\n",
    "Next we evaluate each of our trained models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3819f2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score each classifier and write scores to CSV\n",
    "scores = []\n",
    "for lan_cat in lan_cats:\n",
    "    trainer = joblib.load(f'./models/extended_{lan_cat}_all-mpnet-base-v2.joblib')\n",
    "    test_data = datasets[lan_cat]['test_data']\n",
    "    y_hat = trainer.model(test_data['combo'])\n",
    "    y = test_data['label']\n",
    "    _, fp, fn, tp = confusion_matrix(y_hat, y).ravel()\n",
    "    wf1 = f1_score(y, y_hat, average='weighted')\n",
    "    precision, recall, f1 = nlbse_statistics.get_precision_recall_f1(tp, fp, fn)\n",
    "    scores.append({'lan_cat': lan_cat.lower(),'precision': precision,'recall': recall,'f1': f1,'wf1': wf1})\n",
    "\n",
    "df = pd.DataFrame(scores)\n",
    "df.sort_values('lan_cat').to_excel('scores.xlsx')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df8c95c3",
   "metadata": {},
   "source": [
    "<a id='hub'></a>\n",
    "\n",
    "## Push to hub\n",
    "\n",
    "Finally we push all of our models to the Hugging Face Hub to make them publically avaliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e37f4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Push to hub\n",
    "token = 'hf_XXXXXXXXXXXXXXXXXXXXXXX'\n",
    "repo = 'XXXXXXXXXXXXXXXXXXX'\n",
    "for lan_cat in lan_cats:\n",
    "    trainer = joblib.load(f'./models/{lan_cat}_all-mpnet-base-v2.joblib')\n",
    "    name = lan_cat.lower().replace('_','-') + '-classifier'\n",
    "    print(name)\n",
    "    trainer.push_to_hub(f'{repo}/{name}', use_auth_token=token, private=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
